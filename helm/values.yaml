# Default values for the helm chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

###########
# Deployment and Service
###########
metricsAddress: "0.0.0.0:8888"

minReadySeconds: 5
progressDeadlineSeconds: 120
replicaCount: 1
maxUnavailable: 0

image:
  repository: hypertrace/hypertrace-collector
  pullPolicy: IfNotPresent

env:
  - name: GOGC
    value: "80"

containerPorts:
  - name: grpc-otlp
    containerPort: 4317
  - name: http-otlp
    containerPort: 55681
  - name: grpc-opencensus
    containerPort: 55678
  - name: http-jaeger
    containerPort: 14268
  - name: grpc-jaeger
    containerPort: 14250
  - name: http-zipkin
    containerPort: 9411
  #   Port for exposing internal metrics to prometheus. Should match with {{ .Values.metricsAddress }}
  - name: http-prom-int
    containerPort: 8888
  #   Port for exposing prometheus exporter metrics. Should match with {{ .Values.configmap.data.exporters.prometheus.endpoint }}
  - name: http-prom-exp
    containerPort: 8889

service:
  type: LoadBalancer
  ports:
    - name: grpc-otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: http-otlp
      port: 55681
      targetPort: 55681
      protocol: TCP
    - name: grpc-opencensus
      port: 55678
      targetPort: 55678
      protocol: TCP
    - name: http-jaeger
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: grpc-jaeger
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: http-zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP

livenessProbe:
  initialDelaySeconds: 5
  periodSeconds: 10

readinessProbe:
  initialDelaySeconds: 5
  periodSeconds: 5

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  limits:
    cpu: 1
    memory: 2Gi
  requests:
    cpu: 200m
    memory: 400Mi

deploymentLabels:
  app: hypertrace-collector

podLabels:
  app: hypertrace-collector

podAnnotations: {}

podSecurityContext:
  runAsUser: 65532
  fsGroup: 65532

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
    add:
      - NET_BIND_SERVICE
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault

# The Deployment Selector match labels are different from the pod labels. Note that they should be a subset of the pod
# labels. You append new labels to them but cannot remove labels. If you remove or modify the labels you will need to
# delete the existing deployment bearing the same name and then redeploy. This is the reason why they are separated from
# the pod labels. You can add and remove pod labels without having an effect on the deployment.
# Also, please use "apiVersion: apps/v1" instead of the deprecated "apiVersion: extensions/v1beta1" for the deployment
# apiVersion in the yaml file.
deploymentSelectorMatchLabels:
  app: hypertrace-collector

serviceSelectorLabels:
  app: hypertrace-collector

# Volumes and Volume mounts
volumeMounts:
  - name: hypertrace-collector-config-vol
    mountPath: /conf

volumes:
  - configMap:
      name: hypertrace-collector-config
      items:
        - key: hypertrace-collector-config
          path: hypertrace-collector-config.yaml
    name: hypertrace-collector-config-vol

nodeSelector: {}

###########
# Config Maps
###########
configMap:
  name: hypertrace-collector-config
  data:
    extensions:
      health_check: {}
      pprof:
        endpoint: 0.0.0.0:1777
      zpages:
        endpoint: 0.0.0.0:55679

    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:55681"
      opencensus:
        endpoint: "0.0.0.0:55678"
      zipkin:
        endpoint: "0.0.0.0:9411"
      jaeger:
        protocols:
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
    processors:
      batch: {}
      hypertrace_metrics_resource_attrs_to_attrs: {}
      hypertrace_metrics_remover:
        remove_none_metric_type: true

    exporters:
      kafka:
        protocol_version: 2.0.0
        brokers:
          - bootstrap:9092
        topic: jaeger-spans
        encoding: jaeger_proto
        compression:
          codec: gzip #[none,gzip,snappy,lz4]
          level: 5 #Required only for gzip 1-9
        span_curing:
          enabled: true
          # Drop spans that cannot be cured and will eventually end up being dropped anyway
          # after multiple retries.
          drop_spans: true
        producer:
          compression: gzip
      prometheus:
        endpoint: "0.0.0.0:8889"
        # For converting resource attributes to metric labels we will use the hypertrace_metrics_resource_attrs_to_attrs
        # processor so that we avoid adding job and instance labels as the prometheus exporter will add them.
        # See https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/12cc610f93429fbd9dec71c5f486d266844f11c2/exporter/prometheusexporter/collector.go#L96
        # Once this is fixed we can re-enable this.
        # Filed issue https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/10374
        resource_to_telemetry_conversion:
          enabled: false

    service:
      telemetry:
        logs:
          level: "INFO"
      extensions: [health_check, pprof, zpages]
      pipelines:
        traces:
          receivers: [otlp, opencensus, jaeger, zipkin]
          processors: [batch]
          exporters: [kafka]
        metrics:
          receivers: [otlp]
          processors: [hypertrace_metrics_remover, batch, hypertrace_metrics_resource_attrs_to_attrs]
          exporters: [prometheus]

hpa:
  enabled: false
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80

kafka-topic-creator:
  enabled: true
  jobName: jaeger-spans-kafka-topic-creator
  helmHook: pre-install,pre-upgrade
  kafka:
    topics:
      jaeger-spans:
        replicationFactor: 1
        partitions: 8
        configs:
          retention.bytes: 4294967296
          retention.ms: 86400000
  zookeeper:
    address: zookeeper:2181
  imagePullSecrets: []
  podAnnotations:
    sidecar.istio.io/inject: "false"

podmonitor:
  enabled: false
  interval: 15s
  port: "http-prom-exp"
  scheme: "http"
